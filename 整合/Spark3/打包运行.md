# 运行jar

```bash
ps -ef | grep java

nohup java -jar  xxx.jar 2>&1 > /dev/null &
nohup java -classpath  xxx.jar 2>&1 >  xxx.xxx.classname /dev/null &

```

# Flink on Yarn


[Flink 的三种运行模式](https://niyanchun.com/flink-quick-learning-deployment-mode.html)
[Flink on Yarn两种模式启动参数及在Yarn上的恢复](https://cloud.tencent.com/developer/article/1586186)

[官方参考1](https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/deployment/resource-providers/yarn/)
[官方参考2](https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/deployment/overview/#deployment-modes)

## session 模式

> 第一种模式分为两步：
- yarn-session.sh(开辟资源)
- flink run（提交任务）

```bash
# 开辟资源
yarn-session.sh -n 2 -jm 1024 -tm 1024 -d -s 2

# 参数说明
-n 2 # 表示指定两个容器 
-jm 1024 # 表示jobmanager 1024M内存 
-tm # 1024表示taskmanager 1024M内存 
-d --detached  # 任务后台运行 
-s  # 指定每一个taskmanager分配多少个slots(处理进程)。建议设置为每个机器的CPU核数。一般情况下，vcore的数量等于处理的slot（-s）的数量
-nm,--name # YARN上为一个自定义的应用设置一个名字
-q,--query # 显示yarn中可用的资源 (内存, cpu核数)
-qu,--queue <arg> # 指定YARN队列.
-z,--zookeeperNamespace <arg> # 针对HA模式在zookeeper上创建NameSpace
```
```bash
# 提交任务
./flink run ../examples/batch/WordCount.jar -input hdfs://192.168.83.129:9000/LICENSE -output hdfs://192.168.83.129:9000/wordcount-result.txt
```

## per-job 模式

> 其实也分为两个部分，依然是开辟资源和提交任务，但是在Job模式下，这两步都合成一个命令了

```bash
./flink run -m yarn-cluster -yn 2 -yjm 1024 -ytm 1024 ../examples/batch/WordCount.jar

# 参数
-c,--class <classname> # 如果没有在jar包中指定入口类 则需要在这里通过这个参数指定 
-m,--jobmanager <host:port> # 指定需要连接的jobmanager(主节点)地址 使用这个参数可以指定一个不同于配置文件中的jobmanager 
-p,--parallelism <parallelism> # 指定程序的并行度。可以覆盖配置文件中的默认值
-yjm  # jobmanager内存大小
-ytm  # taskmanager内存大小
-yn # taskmanager个数
-ys    # 一个taskmanager的slot个数
-ynm,--yarnname # 指定任务名称
```

## application 模式

# 部署

> 问题解决

```bash
# parent 打包插件排除的包，可能导致上线运行找不到jar，上线打包要加上

# flink-conf.yaml, 避免POM中的jar 和 flink自带的 冲突
classloader.resolve-order: parent-first
classloader.check-leaked-classloader: false
```

> 部署命令

```bash
# kafka source
nohup java -jar kafkasource-1.0-SNAPSHOT.jar 2>&1 > /dev/null &
# 查看运行情况
ps -ef | grep kafkas*
kafka-console-consumer --bootstrap-server hadoop01:9092  --topic ods_ncddzt

# 建表+action
spark-submit --class top.chendaye666.create.CompactSmallFile \
        --master yarn \
        --name rewrite-smalle-file \
        --jars /opt/work/iceberg-spark3-runtime-0.12.0.jar \
        --deploy-mode cluster \
        --driver-memory 1g \
        --executor-memory 1g \
        --executor-cores 2 \
        /opt/work/sparkicebergddl-1.0-SNAPSHOT.jar

# Per-Job Mode    -yjm,--yarnjobManagerMemory <arg>  (MB)/  -ytm,--yarntaskManagerMemory <arg> (MB)
/opt/flink-1.12.5/bin/flink run  -t yarn-per-job  --detached  /opt/work/datalake/ods-1.0-SNAPSHOT.jar
/opt/flink-1.12.5/bin/flink run -m yarn-cluster -ynm ods  -yjm 1024 -ytm 1024  -ys 2 -d /opt/work/datalake/ods-1.0-SNAPSHOT.jar
/opt/flink-1.12.5/bin/flink run -m yarn-cluster -ynm ods -c top.chendaye666.v2.KafkaToSparkIceberg  -yjm 1024 -ytm 1024 -d /opt/work/datalake/ods-1.0-SNAPSHOT.jar

./bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXXX_YY
./bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_XXXX_YY <jobId>

# Application Mode
/opt/flink-1.12.5/bin/flink run-application -t yarn-application  /opt/work/datalake/ods-1.0-SNAPSHOT.jar
./bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY
./bin/flink cancel -t yarn-application -Dyarn.application.id=application_XXXX_YY <jobId>

/opt/flink-1.12.5/bin/flink run  -t yarn-per-job -h

hdfs dfs -rm -r -skipTrash /warehouse/iceberg
```
